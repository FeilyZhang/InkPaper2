title: "懒惰学习——K最近邻学习算法"
date: 2019-04-02 16:26:50 +0800
update: 2019-04-02 16:26:50 +0800
author: me
cover: "-/images/knn.png"
tags:
    - Machine Learning
preview: k最近邻分类器就是把未标记的案例归类为与它们最相似的带有标记的案例所在的类，广泛适用于数据特征与目标类之间的关系众多且复杂，用其它方式难以理解，但是具有相似类的项目又非常类似的分类任务。

---

KNN(K Nearest Neighbor)是一种非参数和惰性学习算法。非参数意味着没有基础数据分布的假设。换句话说，模型结构由数据集特别是训练集确定。在大多数现实世界数据集不遵循数学理论假设的实践中，这是非常有用的。懒惰算法意味着它不需要任何训练数据点就可以生成模型。在分类器的训练阶段会使用所有的训练集数据。但也就意味着扫描数据点时间的增长和内存占用的提高。

## 一、kNN算法的原理

该算法是基于案例之间的相似度进行分类的，训练集负责训练一个分类器，而测试集直接将案例带入就可以通过k个最相近的训练集案例进行投票以确定最终分类结果。

首先从字面意思来解释，KNN又名K最近邻，有两层含义，分别是

+ K：测试集数据的预测类需要由与它最近的K个邻居投票表决而定，哪类邻居的票数多，该案例就属于哪一类；
+ 最近邻：与测试集数据点距离最近的训练集数据，是为该案例的最近邻。

那么综上所述，K最近邻的本质就是**计算特征空间中测试集案例关于该特征空间中训练集数据点(类)的距离最近的K个训练集数据点(邻居)的距离，测试集案例的预测类通过这K个训练集数据的投票而定，哪一类票数多，测试集案例就属于哪一类**。

### 1.1 距离的衡量

同K均值聚类一样，该算法仍然是基于距离的(也可以说是案例之间的相似度，相似度通过距离来衡量)，距离的计算常用的仍然是欧式距离。公式如下

![](/images/article/k-means1.png)

其中，x和y分别为需要计算的两个案例，xi和yi分别为两个案例相对应的第i个特征的值。这样就足够度量测试集案例同周边K个相邻训练集案例的距离了，也就能找到最近邻。