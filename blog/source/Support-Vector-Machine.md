title: "转导推理——支持向量机"
date: 2019-03-28 19:38:14 +0800
update: 2019-03-28 19:38:14 +0800
author: me
cover: "-/images/svm.jpg"
tags:
    - Machine Learning
preview: 支持向量机基于间隔最大化原理，适用于数值预测以及分类任务。

---

## 一、SVM简介

与其他传统的机器学习方法相比较，支持向量机(Support Vector Machine)的特点如下

1. 以严格的数学理论为基础，克服了传统神经网络学习中靠经验和启发的先验成分等缺点;
2. 以寻找特征空间最优超平面为目标使得学习器具有良好的泛化能力;
3. 用内积的回旋巧妙地构造核函数，克服了特征空间中的维数灾难问题，通过非线性映射，只需在原空间中计算样本数据与支持向量的内积，而不需要知道非线性映射的显性表达式;
4. 成功地解决了小样本学习问题，克服了传统上需要以样本数目无穷多为假设条件来推导各种算法的缺点，得到了小样本条件下的全局最优解。

支持向量机包含三个主要思想：

1. 最优超平面技术(控制决策面的推广能力或泛化能力);
2. 硬间隔支持向量分类(决策边界从正负样本中间穿过)与软间隔支持向量分类(允许间隔计算中存在少许的误差);
3. 内积核函数思想(使解平面从线性扩展到非线性)

## 二、硬间隔支持向量

在数据线性可分的前提下，以分类任务为例，支持向量机可以理解为训练集中由不包括预测类在内的n维向量组成的特征空间中的一个n - 1维的超平面，该平面定义了各个数据点之间的界限。该超平面可以称之为**决策边界**。

高维特征空间不容易想象，不妨以二维特征空间为例，那么划分该二为特征空间的超平面的维数就是1，即一条线。

有如下两种图形，分别是红圈与蓝星，直观上，我们可以用直线B将二者一刀切，正确划分！而直线A与直线C则表现得不是很好，产生了一定的误差。

![](/images/article/svm1.png)

当然，这是少样本的情况，事实上，如果我们愿意，甚至可以画出无数条可以正确区分二者的直线。

![](/images/article/svm2.png)

但是，这是样本较少的情况，如果测试集数据很多，那么我们当前看似能够正确区分二者的直线就有可能无法正确分割。有效应对这种问题的办法就是**寻找两个类之间最大间隔的最大间隔超平面**。

那么在条件**两个类之间最大间隔的最大间隔超平面**的约束下，我们应该选择哪一条作为**决策边界**(超平面)？有两个办法(以本例为例)

第一种办法是，**求两种类边界组成的凸包间最短距离的直线的垂直平分线**

![](/images/article/svm4.png)

上图中分别勾勒出了红圈与蓝星边界构成的凸包，绿色方点虚线为两个凸包之间的最短直线，而直线C则是穿过该最短直线的垂直平分线。那么直线C就是该问题中的最大间隔超平面或者决策边界。为什么一定要是垂直平分线呢？垂直平分线将两个凸包边界的最短距离平分，这样保证决策边界对两个类别是不偏不倚的，如果决策边界位于直线C的左边，那么蓝星就有可能越过该决策边界，同理，如果决策边界位于当前决策边界C的右边，红圈就有可能越过该决策边界。所以，垂直平分最短距离直线的直线才是本例中最好的决策边界。

另一种办法是**通过寻找两个将一组数据划分为同类组的两个平行平面，但这两个平面本身却要尽可能远离，通过这两个平行平面确定决策边界**。

![](/images/article/svm3.png)

如上图所示，我们寻找到了两个能将一组数据划分为同类组的两个平行平面，分别是直线A和B，并且直线A与B已经最大程度上相互远离了，这样的话，我们的决策边界就是从这两个平行平面中间穿过的直线C。

之前说过，支持向量机以严格的数学理论为基础，我们现在的问题是如何给决策边界或者说硬间隔支持向量分类的决策边界下数学定义。

对于直线C，我们设其斜率为k，截距为b，那么其直线方程为

![](/images/article/svm5.png)

同上一篇文章我们推理人工神经网络带0/1输出的硬阈值激活函数的过程相似，斜率k可以看作是特征x的权重，相应的特征y的权重为-1，截距仍然为b。

这里的y并不是单纯的数学上的y，而是特征空间中的另一个特征，真正的y是分类的结果。以上直线方程是建立在二维特征空间的基础之上的，如果将其推广到高维空间，那么就会有如下的表达式

![](/images/article/svm6.png)

其中，`fw,r(x)`代表预测类，即真正的y值，而`W的转置`代表特征空间x向量中各分量的权重，`r`代表截距。这不就是一个多元线性回归方程嘛，可以这么理解，但是不要被回归方程的思维所桎梏。那么我们再重新定义一下，其实`w`和`r`分别对应于把正样本和负样本隔离开的决策边界的法向量和截距。

同上面说的一样，上述方程式在训练集样本完全线性可分的情况下仍然会有无数条直线满足条件，也就意味着分类效果仍然会受训练集样本概括性的影响，其泛化能力取决于测试集的实际情况。因为上述方程并未添加任何约束条件。我们接下来将其约束进一步确保我们的决策边界是唯一的是仅仅在训练集的条件下就具备泛化能力的。




