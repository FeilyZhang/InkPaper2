title: "分而治之——决策树学习算法"
date: 2019-03-23 15:55:25 +0800
update: 2019-03-23 15:55:25 +0800
author: me
cover: "-/images/decision-tree.jpg"
tags:
    - Machine Learning
preview: 决策树学习算法以树形结构建立模型，类似于流程图，该模型本身包含一系列逻辑决策。

---

## 一、简介
决策树学习算法以树形结构建立模型，类似于流程图，该模型本身包含一系列逻辑决策。几个概念如下

+ 决策节点：根据某一特征(属性)作出决定；
+ 决策分支：从决策节点引出的分支表示做出的选择；
+ 终端节点(叶节点)：决策树由叶节点终止，叶节点表示遵循决策组合的结果。

其运行过程为：数据的分类从根节点开始(根节点是第一个决策节点)，根据特征值遍历树上的各个决策节点。数据采用的是一个漏斗形的路径，它将每一条记录汇集到一个节点，在叶节点为该记录分配一个预测类。

## 二、理论原理
决策树是决策函数的可视化描述，决策函数是由训练集的特征空间训练出来的，不同属性的不同程度上的不同组合会形成不同的候选函数(或决策树)，拟合优度因函数(或者不同特征的不同程度上的不同组合)而异，有的明显性能出色，有的明显性能低劣，我们不可能将所有的候选函数全部生成然后一一测试，因为多数情况下随着特征的增加，候选函数呈指数级增长。我们的目的就是找到最优的候选函数，何为最优？浅显易懂的来说就是

> 通过较少的测试达到正确分类，即树中所有路径都较短，整棵树较浅。

要做到路径较短，那么也就意味着每一次的分裂都要是高效的，即每次分裂都可以提高子集的同质性或者纯度，这样减少了不必要的分割，整棵树必然会变浅。那么专业的来说就是

> 每一次决策节点的分裂，得到的子集中的预测类的水平的比例与父集合中的预测类的水平的比例**最大程度上**更低。

## 三、数学定义
决策树算法对以上**最大程度上**的定义是通过**信息增益**实现的。要解释信息增益，先要解释**熵**与**条件熵**。

熵：熵是随机变量的不确定性度量，信息的获取对应于熵的减少。熵越大说明信息量越大该事件内部越不同质，不确定性越大。熵越小说明信息量越小该事件内部越同质，不确定性越小。举个例子

+ 明天早上太阳必然升起。这个事件，即随机变量或该事件没有不确定性，是完全同质的(因为只有这一种可能，还是必然的)；
+ 抛一枚硬币，出现正反面朝上的概率皆为0.5。这个事件的熵为1，即随机变量或该事件具有不确定性，是不同质的(因为样本空间存在两种可能，而不是一种)。

所以，我们可以通过事件的熵来定义该事件内部(样本空间)的同质性或混乱性。熵越大，说明该事件样本空间越不同质越杂乱，实验结果的不确定性更大；熵越小，说明事件样本空间越同质越不杂乱，实验的不确定性更小。其计算公式为
![熵计算公式](/images/article/shang.png)

条件熵：条件熵表示在已知随机变量X的条件下，随机变量Y的不确定性。将其定义为给定X的条件下Y的条件概率分布的熵对X的数学期望。其本质仍然是熵。其计算公式为
![条件熵计算公式](/images/article/tiaojianshang.png)

在决策树算法当中，使用熵值来计算由每一个可能特征的分割所引起的同质性(均匀性)变化，该计算称为信息增益。对于特征F，信息增益的计算方法是分割前的数据分区(S_1)的熵值减去由分割产生的数据分区(S_2)的熵值。其中S_1就是分裂前集合预测类水平的熵值，但是集合被通过该特征划分为了若干个子集，怎样计算它们的熵值呢？

由于一次分割后，数据被划分到了多个分区中，因此计算S_2需要考虑所有分区的熵值总和，这并不意味着简单加总，而是求他们的期望，也就是在分割集合的条件下子集的熵对该次分割的数学期望，即S_2是分割后产生的若干个子集的预测类水平的熵值的期望(即从一个分割得到的总熵就是根据案例落入分区的比例W_i加权的n个分区的熵值的总和)，这样就可以通过条件熵的方式来度量由分割产生的效果，即总熵减少到了多少(因为分割为了多个子集，所以总熵只能通过期望来计算)。这是一种度量方式，但是不够直观，最直观的度量方式就是计算信息增益，如下

由于子集有且只有一个父集(可能会共享同一个父集，但是绝不会同时拥有两个或两个以上的直接父集)，那么也就意味着分割前的熵是相同的，那么分割后的子集的条件熵越小就越说明当前训练集在该选择该特征进行划分后效果最好，因为分割后的总熵(期望熵)越小，即内部越同质越均匀纯度越高。为了便于与其余的特征进行比较，通过计算信息增益，那么也就意味着对同一训练集根据不同特征进行分裂，哪个特征的信息增益越大那么划分的结果越好越同质越均匀越纯。

到此为止，可以看到，决策树算法是通过**信息增益**来确定合理的或最好的分割点的，信息增益在此程度上意味着一次分割获得的信息多少，信息增益越大，那么得到的信息量越多，那么分割的越成功，分割的次数越少，最终树中的所有路径越短，整棵树越浅。信息增益计算公式如下
![信息增益计算公式](/images/article/information-gain.png)

简言之：

**熵越小越说明当前训练集(或子集)内部纯度越高越同质，这是我们的最终目标。刚开始，熵肯定是很大的，我们的目的就是通过根据合适的特征的分裂降低子集的熵值以实现分类预测，而合适的特征的选择是通过信息增益来衡量的**