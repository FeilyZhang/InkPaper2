title: "转导推理——支持向量机"
date: 2019-03-28 19:38:14 +0800
update: 2019-03-28 19:38:14 +0800
author: me
cover: "-/images/svm.jpg"
tags:
    - Machine Learning
preview: 支持向量机基于间隔最大化原理，以严格的数学理论为基础，以寻找特征空间最优超平面为目标，以内积的回旋巧妙地构造核函数，适用于数值预测以及分类任务。

---

## 一、SVM简介

与其他传统的机器学习方法相比较，支持向量机(Support Vector Machine)的特点如下

1. 以严格的数学理论为基础，克服了传统神经网络学习中靠经验和启发的先验成分等缺点;
2. 以寻找特征空间最优超平面为目标使得学习器具有良好的泛化能力;
3. 用内积的回旋巧妙地构造核函数，克服了特征空间中的维数灾难问题，通过非线性映射，只需在原空间中计算样本数据与支持向量的内积，而不需要知道非线性映射的显性表达式;
4. 成功地解决了小样本学习问题，克服了传统上需要以样本数目无穷多为假设条件来推导各种算法的缺点，得到了小样本条件下的全局最优解。

支持向量机包含三个主要思想：

1. 最优超平面技术(控制决策面的推广能力或泛化能力);
2. 硬间隔支持向量分类(决策边界从正负样本中间穿过)与软间隔支持向量分类(允许间隔计算中存在少许的误差);
3. 内积核函数思想(使解平面从线性扩展到非线性)。

## 二、硬间隔支持向量

在数据线性可分的前提下，以分类任务为例，支持向量机可以理解为训练集中由不包括预测类在内的n维向量组成的特征空间中的一个n - 1维的超平面，该平面定义了各个数据点之间的界限。该超平面可以称之为**决策边界**。

### 2.1 从二维空间推广到高维特征空间

高维特征空间比较烧脑，不妨以二维特征空间为例，那么划分该二为特征空间的超平面的维数就是1，即一条线。

有如下两种图形，分别是红圈与蓝星，直观上，我们可以用直线B将二者一刀切，正确划分！而直线A与直线C则表现得不是很好，产生了一定的误差。

![](/images/article/svm1.png)

事实上，如果我们愿意，甚至可以画出无数条可以正确区分二者的直线。

![](/images/article/svm2.png)

但是，这是样本较少的情况，如果测试集数据很多，那么我们当前看似能够正确区分二者的直线就有可能无法正确分割。有效应对这种问题的办法就是**寻找两个类之间最大间隔的最大间隔超平面**。

那么在条件**两个类之间最大间隔的最大间隔超平面**的约束下，我们应该选择哪一条作为**决策边界**(超平面)？有两个办法(以本例为例)：

第一种办法是，**求两种类边界组成的凸包间最短距离的直线的垂直平分线**。

![](/images/article/svm4.png)

上图中分别勾勒出了红圈与蓝星边界构成的凸包，绿色方点虚线为两个凸包之间的最短直线，而直线C则是穿过该最短直线的垂直平分线。那么直线C就是该问题中的最大间隔超平面或者决策边界。为什么一定要是垂直平分线呢？垂直平分线将两个凸包边界的最短距离平分，这样保证决策边界对两个类别是不偏不倚的，如果决策边界位于直线C的左边，那么蓝星就有可能越过该决策边界，同理，如果决策边界位于当前决策边界C的右边，红圈就有可能越过该决策边界。所以，垂直平分最短距离直线的直线才是本例中最好的决策边界。

另一种办法是**通过寻找两个将一组数据划分为同类组的两个平行平面，但这两个平面本身却要尽可能远离，通过这两个平行平面确定决策边界**。

![](/images/article/svm3.png)

如上图所示，我们寻找到了两个能将一组数据划分为同类组的两个平行平面，分别是直线A和B，并且直线A与B已经最大程度上相互远离了，这样的话，我们的决策边界就是从这两个平行平面中间穿过的直线C。

之前说过，支持向量机以严格的数学理论为基础，我们现在的问题是如何给决策边界或者说硬间隔支持向量分类的决策边界下数学定义。

对于直线C，我们设其斜率为k，截距为b，那么其直线方程为：

![](/images/article/svm5.png)

同上一篇文章我们推理人工神经网络带0/1输出的硬阈值激活函数的过程相似，斜率k可以看作是特征x的权重，相应的特征y的权重为-1，截距仍然为b。

这里的y并不是单纯的数学上的y，而是特征空间中的另一个特征，真正的y是分类的结果。以上直线方程是建立在二维特征空间的基础之上的，如果将其推广到高维空间，那么就会有如下的表达式：

![](/images/article/svm6.png)

其中，`fw,r(x)`代表预测类，即真正的y值，而`W的转置`代表特征空间x向量中各分量的权重，`r`代表截距。这不就是一个多元线性回归方程嘛，可以这么理解，但是不要被回归方程的思维所桎梏。那么我们再重新定义一下，其实`w`和`r`分别对应于把正样本和负样本隔离开的决策边界的法向量和截距。

同上面说的一样，上述方程式在训练集样本完全线性可分的情况下仍然会有无数条直线满足条件，也就意味着分类效果仍然会受训练集样本概括性的影响，其泛化能力取决于测试集的实际情况。因为上述方程并未添加任何约束条件。我们接下来将其约束进一步确保我们的决策边界是唯一的是仅仅在训练集的条件下就具备泛化能力的。

之前说过，决策边界是由两个平行平面决定的，也就意味着方程式斜率(特征空间各特征分量的权重)是一致的，即`w`相同，由于是二分类问题，我们定义正样本的边界(非决策边界而是正样本的与决策边界相平行的平行平面)为：

![](/images/article/svm7.png)

那么同理，负样本的边界定义为：

![](/images/article/svm8.png)

也就是说，所有的正样本被定义为1，所有的负样本被定义为-1，可以将这里的1与-1理解为训练集样本的实际值，但是为什么一定要定义为-1和1呢？这是因为一正一负最容易处理，而+1和-1更容易处理。

综上，就有如下表达式：

![](/images/article/svm9.png)

其中`fw,r(x)`表示根据特征空间得到的预测值，而`yi`表示训练集样本目标值，由于我们上述将正样本的目标值定义为了`+1`，那么如果正样本的特征向量带入`fw,r(x)`后，得到的值为正，那么`fw,r(x)yi`一定为正；同理，我们上述将负样本的目标值定义为了`-1`，那么如果负样本的特征向量带入`fw,r(x)`后，得到的值为负，那么`fw,r(x)yi`也一定为正；也就是说：

**如果模型正确二分类，那么上述表达式的值一定是大于0的。**

我们现在找到了正确二分类的条件，但这还是远远不够的，同样的理由：可以正确二分类的函数有若干个(对应于几何解释就是有若干条直线可以正确划分正负样本训练集)，但是能最佳二分类的函数只有一个(对应的集合解释就是两个类之间最大间隔的最大间隔超平面只有一个)，那么接下来的目标就是定义这个最佳的超平面。

而且我们也看出了，只有将正负样本的`yi`设置为异号时，`fw,r(x)yi`才能保证大于0，当然，也可以将正样本定义为-1，负样本定义为1，这样上述表达式的不等式就应该是小于0，很明显，这样的定义有点反人类。

继续回到上述表达式(不等式)。该表达式表示了这个模型可以对所有的训练样本(训练集样本定义如下图所示)进行正确的分类：

![](/images/article/svm10.png)

由于如下开集约束条件在数学上难以处理：

![](/images/article/svm11.png)

而利用参数w和r可以任意取值的性质，可以将其变换为闭集约束条件：

![](/images/article/svm12.png)

`fw,r(x)yi > 0`与`fw,r(x)yi > 1`的意义是一样的。即：**满足该条件的样本是可以被正确二分类的**。我们也可以换一种说法，即：**当存在满足这样的条件的(w, r)时，就可以称这样的训练样本为线性可分的样本**。

对于线性可分的训练样本，可以将所有的**训练样本**都正确分类的解(超平面或决策边界)有无数个。我们的目标就是找到唯一的最佳超平面，这样才能让我们的模型在测试样本上同样具备良好的鲁棒性。

事实上，我们要做的就是**使得训练集样本数据点最接近决策边界的数据点到决策边界的距离(或间隔)最大**，这是我们的终极目标。这句话有两层含义：

+ 训练集样本数据点最接近决策边界的数据点；
+ 该数据点(训练集样本数据点最接近决策边界的数据点)到决策边界的距离最大。

首先看第一层含义，**训练集样本数据点最接近决策边界的数据点**，这不就是求训练集样本到超平面的距离的最小值吗？即，我们如下定义点到超平面的距离(就是点到面的距离公式在高维空间的推广)：

![](/images/article/svm13.png)

那么训练集样本到超平面的距离的最小值就可以定义为：

![](/images/article/svm14.png)

由于上式分子大于等于1，那么就等同于

![](/images/article/svm15.png)

这样，我们就定义了训练集样本数据点最接近决策边界的数据点。

接下来看第二层含义，**该数据点(训练集样本数据点最接近决策边界的数据点)到决策边界的距离最大**，也就意味着将上式最大化，即

![](/images/article/svm16.png)

由于距离必然是正数，那么也就是说上式得图像就是一个定义域为`(0, +∞)`的反比例函数，让反比例函数值最大，也就意味着距离最小，即

![](/images/article/svm17.png)

如果距离的平方最小，那么距离必然最小，即上式等价于

![](/images/article/svm18.png)

这样的话，通过上述条件，我们就找到了最佳超平面，但是别忘了，上述条件我们是在训练集样本是线性可分的条件下推导的(因为我们上面定义了在满足`fw,r(x)yi > 1`的条件时，我们就说**当存在这样的条件的(w, y)时，这样的训练集样本为线性可分的样本**，然后我们才在此基础之上推导的)，也就是说上述条件的约束条件就是训练集样本线性可分，即

![](/images/article/svm19.png)

上述公式即为**使得训练集样本数据点最接近决策边界的数据点到决策边界的距离(或间隔)最大所对应的分类器**，称之为**硬间隔支持向量机**。公式的含义就是在条件`fw,r(x)yi > 1`的约束下或存在这样的条件的(w, y)时训练集样本线性可分的约束下硬间隔支持向量机(或训练集样本数据点最接近决策边界的数据点到决策边界的距离(或间隔)最大所对应的分类器)的决策边界。

### 2.2 硬间隔支持向量的完整数学定义
